{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6868189,"sourceType":"datasetVersion","datasetId":3937441},{"sourceId":7328792,"sourceType":"datasetVersion","datasetId":4254030},{"sourceId":7329851,"sourceType":"datasetVersion","datasetId":4254748},{"sourceId":7329935,"sourceType":"datasetVersion","datasetId":4254812},{"sourceId":7331325,"sourceType":"datasetVersion","datasetId":4255808},{"sourceId":2622,"sourceType":"modelInstanceVersion","modelInstanceId":1899},{"sourceId":2938,"sourceType":"modelInstanceVersion","modelInstanceId":2180},{"sourceId":5906,"sourceType":"modelInstanceVersion","modelInstanceId":4679}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom tensorflow import keras\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-01-03T14:20:39.754134Z","iopub.execute_input":"2024-01-03T14:20:39.754663Z","iopub.status.idle":"2024-01-03T14:20:39.774647Z","shell.execute_reply.started":"2024-01-03T14:20:39.754624Z","shell.execute_reply":"2024-01-03T14:20:39.773698Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"/kaggle/input/bert/keras/bert_base_en/1/config.json\n/kaggle/input/bert/keras/bert_base_en/1/tokenizer.json\n/kaggle/input/bert/keras/bert_base_en/1/metadata.json\n/kaggle/input/bert/keras/bert_base_en/1/model.weights.h5\n/kaggle/input/bert/keras/bert_base_en/1/assets/tokenizer/vocabulary.txt\n/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-128-a-2/2/saved_model.pb\n/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-128-a-2/2/keras_metadata.pb\n/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-128-a-2/2/assets/vocab.txt\n/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-128-a-2/2/variables/variables.index\n/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-128-a-2/2/variables/variables.data-00000-of-00001\n/kaggle/input/bert/tensorflow2/en-uncased-preprocess/3/saved_model.pb\n/kaggle/input/bert/tensorflow2/en-uncased-preprocess/3/keras_metadata.pb\n/kaggle/input/bert/tensorflow2/en-uncased-preprocess/3/assets/vocab.txt\n/kaggle/input/bert/tensorflow2/en-uncased-preprocess/3/variables/variables.index\n/kaggle/input/bert/tensorflow2/en-uncased-preprocess/3/variables/variables.data-00000-of-00001\n/kaggle/input/llm-generated-essays/ai_generated_train_essays.csv\n/kaggle/input/llm-generated-essays/ai_generated_train_essays_gpt-4.csv\n/kaggle/input/finetuned-data/finetuned.csv\n/kaggle/input/encoderbert/encoder/saved_model.pb\n/kaggle/input/encoderbert/encoder/keras_metadata.pb\n/kaggle/input/encoderbert/encoder/assets/vocab.txt\n/kaggle/input/encoderbert/encoder/variables/variables.index\n/kaggle/input/encoderbert/encoder/variables/variables.data-00000-of-00001\n/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\n/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\n/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\n/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\n/kaggle/input/bertpreprocess/bertpreprocess/saved_model.pb\n/kaggle/input/bertpreprocess/bertpreprocess/keras_metadata.pb\n/kaggle/input/bertpreprocess/bertpreprocess/assets/vocab.txt\n/kaggle/input/bertpreprocess/bertpreprocess/variables/variables.index\n/kaggle/input/bertpreprocess/bertpreprocess/variables/variables.data-00000-of-00001\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 1 PREPARE THE DATA\n","metadata":{}},{"cell_type":"code","source":"t1 = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\nt2 = pd.read_csv('/kaggle/input/llm-generated-essays/ai_generated_train_essays_gpt-4.csv')\nt3 = pd.read_csv('/kaggle/input/llm-generated-essays/ai_generated_train_essays.csv')\ntrain_dataset=pd.concat([t1,t2,t3],ignore_index=True)\ntrain_dataset.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:20:39.776190Z","iopub.execute_input":"2024-01-03T14:20:39.776476Z","iopub.status.idle":"2024-01-03T14:20:39.850268Z","shell.execute_reply.started":"2024-01-03T14:20:39.776450Z","shell.execute_reply":"2024-01-03T14:20:39.849381Z"},"trusted":true},"execution_count":114,"outputs":[{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"(2078, 4)"},"metadata":{}}]},{"cell_type":"code","source":"t1.generated.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:20:39.852409Z","iopub.execute_input":"2024-01-03T14:20:39.852734Z","iopub.status.idle":"2024-01-03T14:20:39.860384Z","shell.execute_reply.started":"2024-01-03T14:20:39.852707Z","shell.execute_reply":"2024-01-03T14:20:39.859331Z"},"trusted":true},"execution_count":115,"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"generated\n0    1375\n1       3\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset.to_csv('/kaggle/working/data.csv')\ntrain_dataset.generated.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:20:39.863308Z","iopub.execute_input":"2024-01-03T14:20:39.863996Z","iopub.status.idle":"2024-01-03T14:20:40.165776Z","shell.execute_reply.started":"2024-01-03T14:20:39.863959Z","shell.execute_reply":"2024-01-03T14:20:40.164829Z"},"trusted":true},"execution_count":116,"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"generated\n0    1375\n1     703\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Howeverï¼Œthere are some problems with the text, so we rectify them and get the improved data-set, finetune.csv.","metadata":{}},{"cell_type":"code","source":"train_dataset=pd.read_csv(\"/kaggle/input/finetuned-data/finetuned.csv\")\ntrainset,validset=train_test_split(train_dataset, test_size=0.2, random_state=42)\ntrainset.to_csv('/kaggle/working/trainset.csv')\nvalidset.to_csv('/kaggle/working/validset.csv')\nprint(trainset.shape,validset.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:20:40.168430Z","iopub.execute_input":"2024-01-03T14:20:40.168732Z","iopub.status.idle":"2024-01-03T14:20:40.541488Z","shell.execute_reply.started":"2024-01-03T14:20:40.168706Z","shell.execute_reply":"2024-01-03T14:20:40.540483Z"},"trusted":true},"execution_count":117,"outputs":[{"name":"stdout","text":"(1662, 7) (416, 7)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2 NERUAL NETWORK CONSTRUCTION","metadata":{}},{"cell_type":"code","source":"from typing import Any\nimport torch\nimport re\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nimport sys\n\n\nclass perplexity:\n    def __init__(\n        self, device=\"cpu\", model=\"gpt2\",tokenizer='/kaggle/working/mytok', threshold=80, separately=True\n    ):\n        \"\"\"\n        Initializes the Perplexity class with the specified parameters.\n\n        Parameters:\n        - device: str, default=\"cpu\". Device to use for the model (e.g., \"cpu\" or \"cuda\").\n        - model: str, default=\"gpt2\". Pretrained model to use for tokenization and language modeling.\n        - threshold: int, default=80. Threshold for the second perplexity range.\n        - separately: bool, default=True. If True, evaluates perplexity separately for each line in the input text.\n        \"\"\"\n        self.device = device\n        self.tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer)\n        self.model = GPT2LMHeadModel.from_pretrained(model).to(device)\n        self.max_length = self.model.config.n_positions\n        self.threshold = threshold\n        self.stride = 50\n        self.method = separately\n\n    def sentenceppl(self, sentence):\n        \"\"\"\n        Computes perplexity for a given sentence.\n\n        Parameters:\n        - sentence: str. Input sentence for perplexity computation.\n\n        Returns:\n        - ppl: int. Perplexity value for the input sentence.\n        \"\"\"\n        last_end = 0\n        encodings = self.tokenizer(sentence, return_tensors=\"pt\")\n        len_sentence = encodings.input_ids.size(1)\n        last_end = 0\n        begin = 0\n        nll_column = []\n        nlls = []\n\n        while last_end < len_sentence:\n            end = min(begin + self.max_length, len_sentence)\n            target_len = end - begin\n            input_ids = encodings.input_ids[:, begin:end].to(self.device)\n            target_ids = input_ids.clone()\n            target_ids[:, -target_len] = -100\n            with torch.no_grad():\n                output = self.model(input_ids, labels=target_ids)\n                nll_unit = output.loss * target_len\n                nll_column.append(nll_unit)\n            nlls.append(nll_column)\n            begin = begin + self.stride\n            last_end = end\n        ppl = torch.exp(\n            torch.stack([torch.Tensor(nll) for nll in nlls]).sum() / len_sentence\n        )\n        if torch.isnan(ppl):\n            return ppl\n        else:\n            return int(ppl)\n\n    def result(self, value):\n        \"\"\"\n        Determines the result label based on the perplexity value.\n\n        Parameters:\n        - value: int. Perplexity value.\n\n        Returns:\n        - result: str. Result label description.\n        - label: int. Result label (0 or 1).\n        \"\"\"\n        if value < self.threshold:\n            label = 0\n            return \"The paragraph is most likely generated by AI\", label\n        else:\n            label = 1\n            return \"This paragraph is most likely artificial\", label\n\n    def __call__(self, text):\n        \"\"\"\n        Evaluates the input text and prints the result label.\n\n        Parameters:\n        - text: str. Input text for evaluation.\n\n        Returns:\n        - label: int. Result label (0 or 1).\n        \"\"\"\n        valid_text = re.findall(\"[a-zA-Z0-9]+\", text)\n        valid_length = sum([len(i) for i in valid_text])\n        \n\n        if self.method:\n            lines = re.split(r\"(?<=[.?!][ \\[\\(])|(?<=\\n)\\s*\", text)\n            lines = list(filter(lambda x: (x is not None) and (len(x) > 0), lines))\n            offset = \"\"\n            perlineppl = []\n            for line in lines:\n                if re.search(\"[a-zA-Z0-9]+\", line) == None:\n                    continue\n                if len(offset) > 0:\n                    line = offset + line\n                    offset = \"\"\n                # remove the new line pr space in the first sentence if exists\n                if line[0] == \"\\n\" or line[0] == \" \":\n                    line = line[1:]\n                if line[-1] == \"\\n\" or line[-1] == \" \":\n                    line = line[:-1]\n                elif line[-1] == \"[\" or line[-1] == \"(\":\n                    offset = line[-1]\n                    line = line[:-1]\n                ppl = self.sentenceppl(line)\n                perlineppl.append(ppl)\n            ppl = sum(perlineppl) / len(perlineppl)\n            result, label = self.result(ppl)\n            # print(result)\n            # print(ppl)\n            return label#,ppl\n        else:\n            ppl = self.sentenceppl(text)\n            result, label = self.result(ppl)\n            # print(result)\n            # print(ppl)\n            return label#,ppl\n        \n    def predict(self,texts):\n        result=[]\n        for text in texts:\n            a=self(text)\n            if a==0:\n                result.append(1)\n            elif a==1:                    \n                result.append(0)\n            else:\n                raise Exception\n        return np.array(result)\n                \n","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:20:40.542843Z","iopub.execute_input":"2024-01-03T14:20:40.543159Z","iopub.status.idle":"2024-01-03T14:20:40.565220Z","shell.execute_reply.started":"2024-01-03T14:20:40.543132Z","shell.execute_reply":"2024-01-03T14:20:40.564288Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"model=perplexity(device='cuda',threshold=15,separately=False,model='/kaggle/input/mymodel/mymod',tokenizer='/kaggle/input/mymodel/mytok')\n# Load datasets\ntrainset = pd.read_csv('/kaggle/working/trainset.csv')\nvalidset = pd.read_csv('/kaggle/working/validset.csv')\ntest_data = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n\n# Process datasets\nX_train_text = trainset['text']\nX_val_text = validset['text']\nX_test_text = test_data['text']\n\ny_train_labels = trainset['generated']\ny_val_labels = validset['generated']\n\n# Convert Pandas Series to numpy arrays\nX_train_text_array = np.array(X_train_text)\nX_val_text_array = np.array(X_val_text)\ny_train_labels_array = np.array(y_train_labels)\ny_val_labels_array = np.array(y_val_labels)\n\n# Preprocess test set\nX_test_text_array = np.array(X_test_text)\n\n# Make predictions on the test set\ny_test_predictions = pd.Series(model.predict(X_test_text_array).flatten(), name='generated')\n\n# Concatenate ID and predictions and save to CSV\noutput_dataframe = pd.concat((test_data['id'], y_test_predictions), axis=1)\noutput_dataframe.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:20:40.566545Z","iopub.execute_input":"2024-01-03T14:20:40.567214Z","iopub.status.idle":"2024-01-03T14:20:41.408523Z","shell.execute_reply.started":"2024-01-03T14:20:40.567187Z","shell.execute_reply":"2024-01-03T14:20:41.407709Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"# from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n# model=GPT2LMHeadModel.from_pretrained('gpt2')\n# tokenizer=GPT2TokenizerFast.from_pretrained('gpt2')\n# %mkdir mytok\n# %mkdir mymod\n# tokenizer.save_pretrained('./mytok')\n# model.save_pretrained('./mymod')","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:20:41.409804Z","iopub.execute_input":"2024-01-03T14:20:41.410167Z","iopub.status.idle":"2024-01-03T14:20:41.414562Z","shell.execute_reply.started":"2024-01-03T14:20:41.410136Z","shell.execute_reply":"2024-01-03T14:20:41.413625Z"},"trusted":true},"execution_count":120,"outputs":[]}]}